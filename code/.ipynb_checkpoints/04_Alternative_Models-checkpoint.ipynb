{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Classifying clinically actionable genetic mutations\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 4: Alternative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code to explore alternative models to address the classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "- [Importing of Libraries](#Importing-of-Libraries)\n",
    "- [Data Import](#Data-Import)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing of Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tabulate import tabulate\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "TRAIN_SET_PATH = \"../assets/train_prep.csv\"\n",
    "\n",
    "GLOVE_6B_50D_PATH = \"../assets/glove.6B.50d.txt\"\n",
    "GLOVE_6B_300D_PATH = \"../assets/glove.6B.300d.txt\"\n",
    "encoding=\"utf-8\"\n",
    "\n",
    "BERT_INIT_CHKPNT = \"../assets/cased_L-12_H-768_A-12/bert.model.ckpt.index\"\n",
    "BERT_VOCAB = \"../assets/cased_L-12_H-768_A-12/vocab.txt\"\n",
    "BERT_CONFIG = \"../assets/cased_L-12_H-768_A-12/bert_config.json\"\n",
    "\n",
    "from sklearn import linear_model, metrics, svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,\\\n",
    "    cross_val_score, RandomizedSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize, MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, roc_auc_score,\\\n",
    "    classification_report, f1_score, balanced_accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import cycle\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import regex as re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, GRU, LSTM, Embedding, SpatialDropout1D, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "#To make tf 2.0 compatible with tf1.0 code, we disable the tf2.0 functionalities\n",
    "tf.disable_eager_execution()\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "from bert import modeling\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "# Initialise random seeed for more consistent results\n",
    "from numpy.random import seed\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'train_prep' and 'test_prep' datasets\n",
    "# We use the 'keep_default_na' option to False to ensure that pandas does not re-introduce missing values\n",
    "train = pd.read_csv(\"../assets/train_prep.csv\", keep_default_na=False)\n",
    "test = pd.read_csv(\"../assets/test_prep.csv\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3321, 4325), (986, 4324))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>gene_ABCB11</th>\n",
       "      <th>gene_ABCC6</th>\n",
       "      <th>gene_ABL1</th>\n",
       "      <th>gene_ACVR1</th>\n",
       "      <th>gene_ADAMTS13</th>\n",
       "      <th>gene_ADGRG1</th>\n",
       "      <th>gene_AGO2</th>\n",
       "      <th>...</th>\n",
       "      <th>variation_YAP1-TFE3 Fusion</th>\n",
       "      <th>variation_YWHAE-ROS1 Fusion</th>\n",
       "      <th>variation_ZC3H7B-BCOR Fusion</th>\n",
       "      <th>variation_ZNF198-FGFR1 Fusion</th>\n",
       "      <th>variation_null1313Y</th>\n",
       "      <th>variation_null189Y</th>\n",
       "      <th>variation_null262Q</th>\n",
       "      <th>variation_null267R</th>\n",
       "      <th>variation_null399R</th>\n",
       "      <th>variation_p61BRAF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>cyclin dependent kinase cdks regulate variety ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>abstract background non small lung nsclc heter...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 4325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  class                                               text  gene_ABCB11  \\\n",
       "0   0      1  cyclin dependent kinase cdks regulate variety ...            0   \n",
       "1   1      2  abstract background non small lung nsclc heter...            0   \n",
       "\n",
       "   gene_ABCC6  gene_ABL1  gene_ACVR1  gene_ADAMTS13  gene_ADGRG1  gene_AGO2  \\\n",
       "0           0          0           0              0            0          0   \n",
       "1           0          0           0              0            0          0   \n",
       "\n",
       "   ...  variation_YAP1-TFE3 Fusion  variation_YWHAE-ROS1 Fusion  \\\n",
       "0  ...                           0                            0   \n",
       "1  ...                           0                            0   \n",
       "\n",
       "   variation_ZC3H7B-BCOR Fusion  variation_ZNF198-FGFR1 Fusion  \\\n",
       "0                             0                              0   \n",
       "1                             0                              0   \n",
       "\n",
       "   variation_null1313Y  variation_null189Y  variation_null262Q  \\\n",
       "0                    0                   0                   0   \n",
       "1                    0                   0                   0   \n",
       "\n",
       "   variation_null267R  variation_null399R  variation_p61BRAF  \n",
       "0                   0                   0                  0  \n",
       "1                   0                   0                  0  \n",
       "\n",
       "[2 rows x 4325 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>gene_ABCB11</th>\n",
       "      <th>gene_ABCC6</th>\n",
       "      <th>gene_ABL1</th>\n",
       "      <th>gene_ACVR1</th>\n",
       "      <th>gene_ADAMTS13</th>\n",
       "      <th>gene_ADGRG1</th>\n",
       "      <th>gene_AGO2</th>\n",
       "      <th>gene_AGXT</th>\n",
       "      <th>...</th>\n",
       "      <th>variation_YAP1-TFE3 Fusion</th>\n",
       "      <th>variation_YWHAE-ROS1 Fusion</th>\n",
       "      <th>variation_ZC3H7B-BCOR Fusion</th>\n",
       "      <th>variation_ZNF198-FGFR1 Fusion</th>\n",
       "      <th>variation_null1313Y</th>\n",
       "      <th>variation_null189Y</th>\n",
       "      <th>variation_null262Q</th>\n",
       "      <th>variation_null267R</th>\n",
       "      <th>variation_null399R</th>\n",
       "      <th>variation_p61BRAF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>incidence breast increase china recent decade ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>unselected series colorectal carcinoma stratif...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 4324 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  gene_ABCB11  \\\n",
       "0   1  incidence breast increase china recent decade ...            0   \n",
       "1   2  unselected series colorectal carcinoma stratif...            0   \n",
       "\n",
       "   gene_ABCC6  gene_ABL1  gene_ACVR1  gene_ADAMTS13  gene_ADGRG1  gene_AGO2  \\\n",
       "0           0          0           0              0            0          0   \n",
       "1           0          0           0              0            0          0   \n",
       "\n",
       "   gene_AGXT  ...  variation_YAP1-TFE3 Fusion  variation_YWHAE-ROS1 Fusion  \\\n",
       "0          0  ...                           0                            0   \n",
       "1          0  ...                           0                            0   \n",
       "\n",
       "   variation_ZC3H7B-BCOR Fusion  variation_ZNF198-FGFR1 Fusion  \\\n",
       "0                             0                              0   \n",
       "1                             0                              0   \n",
       "\n",
       "   variation_null1313Y  variation_null189Y  variation_null262Q  \\\n",
       "0                    0                   0                   0   \n",
       "1                    0                   0                   0   \n",
       "\n",
       "   variation_null267R  variation_null399R  variation_p61BRAF  \n",
       "0                   0                   0                  0  \n",
       "1                   0                   0                  0  \n",
       "\n",
       "[2 rows x 4324 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting of data into Predictor (X) and Target (y) Dataframes\n",
    "X = train[[i for i in train.columns if i not in ['id', 'class']]]\n",
    "y = train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3321, 4323), (3321,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save copies of the original X and y dataframes so that we can try different word embeddings as needed\n",
    "X_original = X.copy()\n",
    "y_original = y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(986, 4323)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# For convenience, we create an array called 'X_all_text that contains\n",
    "# all the text from our training set, across all samples\n",
    "X_all_text = []\n",
    "for text in X['text']:\n",
    "    X_all_text.append(text.split())\n",
    "X_all_text = np.array(X_all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3321,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of all the unique words from our taining set, across all samples\n",
    "X_unique_words = set()\n",
    "for row in range(len(X_all_text)):\n",
    "    X_unique_words.update(X_all_text[row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81312"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_phrase_len = 0\n",
    "for row in range(len(X_all_text)):\n",
    "    if len(X_all_text[row]) > max_phrase_len:\n",
    "        max_phrase_len = len(X_all_text[row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44019"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_phrase_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring alternative word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we evaluate various word embeddings other than the ones created using the standard TfidfVectorizer as used in the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manage the overall processing power needed to find the best alternative model, we seek to find the best alternative model based on the folowing steps:\n",
    "1. Find an alternative set of word embeddings but use a fixed classifier (Extra Trees Classifier) to measure their performance against our validation dataset\n",
    "2. Find the best classifier for our chosen word embeddings\n",
    "\n",
    "For **Step 1** above, we explore three static word embeddings to see if we can get better classification results:\n",
    "- Global Vectors for Word Representation (GloVe) pre-trained word embeddings created by the [Stanford Natural Language Processing Group](https://nlp.stanford.edu/) and available at https://nlp.stanford.edu/projects/glove/:\n",
    "    - A smaller set of GloVe embeddings (which we call **'glove_small'**) that are based on based on Wikipedia 2015 and Gigaword 5th Edition (https://catalog.ldc.upenn.edu/LDC2011T07)\n",
    "    - A larger set of GloVe embeddings (which we call **'glove_big'**) that are based on Common Crawl (https://commoncrawl.org/)\n",
    "- Our own word embeddings created by training Word2Vec (from nltk) on all the given text in the training dataset, which we call **'w2v'**.\n",
    "\n",
    "For **Step 2** above, we evaluate the performance of a Forward Neural Network (FNN) and an Extra Trees Classifier. For both classifiers, we use RandomisedSearchCV to find the set of optimal parameters to give the best cross-validated score based on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of pre-trained word embeddings based on GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the two pre-trained word embeddings (based on GloVe) and store them as dictionary objects 'glove_small' and 'glove_big'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import struct \n",
    "\n",
    "# glove_small keys are bounded by what is common to both the X_all_text and the glove file\n",
    "glove_small = {}\n",
    "all_words = set(w for words in X_all_text for w in words)\n",
    "with open(GLOVE_6B_50D_PATH, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        if (word in all_words):\n",
    "            nums=np.array(parts[1:], dtype=np.float32)\n",
    "            glove_small[word] = nums\n",
    "\n",
    "# glove_big keys are bounded by what is common to both the X_all_text and the glove file\n",
    "glove_big = {}\n",
    "with open(GLOVE_6B_300D_PATH, \"rb\") as infile:\n",
    "    for line in infile:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode(encoding)\n",
    "        if word in all_words:\n",
    "            nums=np.array(parts[1:], dtype=np.float32)\n",
    "            glove_big[word] = nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of ELMo word embeddings based on training dataset vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the ELMo module (version 3) from TensorFlow Hub. The ELMo module computes contextualized word representations using character-based word representations and bidirectional LSTMs. \n",
    "\n",
    "The module supports inputs both in the form of raw text strings or tokenized text strings. It outputs fixed embeddings at each LSTM layer, a learnable aggregation of the 3 layers, and a fixed mean-pooled vector representation of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that will extract the ELMo vectors of all the words in a single sample and take their mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_vectors(x):\n",
    "    embeddings = elmo(x, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        # return average of ELMo features\n",
    "        return sess.run(tf.reduce_mean(embeddings,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now call the function for each sample text in the X_all_text list we have created earlier. Due to memory constraints, we cannot store the entire set of ELMo vectors in memory. Instead, we write them to disk using the Pickle serialisation library using the most efficient Pickle serialisation protocol available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "num_rows = len(X_all_text)\n",
    "block_size = 500\n",
    "first_start = time.time()\n",
    "for row in range(num_rows):\n",
    "    row_start = time.time()\n",
    "    df = pd.DataFrame(elmo_vectors(X_all_text[row]))\n",
    "    output_file_name = \"../assets/elmo_vectors_\" + str((row//block_size)*block_size) + \".csv\"\n",
    "    df.to_csv(output_file_name, mode='a')\n",
    "    print (\"Row {} of {} appended, duration (h:m:s): {}, total elapsed time (h:m:s): {}\".format(row+1, num_rows+1,\n",
    "               str(datetime.timedelta(seconds=time.time()-row_start)),\n",
    "               str(datetime.timedelta(seconds=time.time()-first_start))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of Word2Vec word embeddings based on training dataset vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create our own word embeddings using Word2Vec from nltk, trained using all the text in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec(X_all_text, size=100, window=5, min_count=5, workers=8)\n",
    "w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining new Vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We define two word embedding vectorizers:\n",
    "- **MeanEmbeddingVectorizer**: takes the mean of all the 'glove_small' vectors corresponding to individual words\n",
    "- **TfidfEmbeddingVectorizer** takes the mean of all the 'glove_small' vectors corresponding to individual words **weighted** based on each word's inverse document frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    \n",
    "    # Creating a new MeanEmbeddingVectorizer object with input parameter of 'word2vec'\n",
    "    # will create a new array of size equal to the number of words contained in word2vec\n",
    "    # that are also present in the glove_small dictionary\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "            \n",
    "    # The 'fit' method simply returns the input parameter 'word2vec' itself\n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    # The 'transform' method returns an array consisting of the mean of all\n",
    "    # 'glove_small\" vectors that correspond to individual words in the input parameter 'X'\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "# TfidfVectorizer is a variation of the Term Frequency - Inverse Document Frequency (Tfidf) Vectorizer\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    \n",
    "    # Creating a new TfidfEmbeddingVectorizer object with input parameter of 'word2vec'\n",
    "    # will create an array of size equal to the number of words contained in word2vec\n",
    "    # that are also present in the glove_small dictionary\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(glove_small))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "        \n",
    "    # The 'fit' method updates the word weights attribute based on the\n",
    "    # output of the standard TfidfVectorizer\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    # The 'transform' method returns an array consisting of the mean of all 'glove_small'\n",
    "    # vectors (adjusted by the Tfidf weights) that correspond to individual words\n",
    "    # in the input parameter 'X'\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best combination of word embeddings and vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify the best combination of the alternative word embeddings above and the vectorizers defined above, we calculate cross-validated accuracy scores for different pipelines (i.e. combinations) of the word embeddings and vectorizers, coupled with the Extra Trees classifier. The Extra Trees Classifier is chosen based on its generally good performance; it produces less variance but may cause an increase in bias, compared to a Random Forest classifier.\n",
    "\n",
    "Our goal is to pin down our chosen word embeddings and vectorizer for subsequent training of the classifier.\n",
    "\n",
    "The different pipelines are as follows:\n",
    "- mev_w2v: MeanEmbeddingVectorizer based on our own 'w2v' word vectors created using Word2Vec on the words in the training dataset\n",
    "- tev_w2v: TfidfEmbeddingVectorizer based on our own 'w2v word vectors\n",
    "- mev_glove_small: MeanEmbeddingVectorizer based on the 'glove_small' word vectors\n",
    "- tev_glove_small: TfidfEmbeddingVectorizer based on the 'glove_small' word vectors\n",
    "- mev_glove_big: MeanEmbeddingVectorizer based on the 'glove_big' word vectors\n",
    "- tev_glove_big: TfidfEmbeddingVectorizer based on the 'glove_big' word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the various pipelines for evaluation\n",
    "mev_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200, verbose=1, random_state=42, n_jobs=-1))])\n",
    "tev_w2v = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200, verbose=1, random_state=42, n_jobs=-1))])\n",
    "mev_glove_small = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_small)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200, verbose=1, random_state=42, n_jobs=-1))])\n",
    "tev_glove_small = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_small)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200, verbose=1, random_state=42, n_jobs=-1))])\n",
    "mev_glove_big = Pipeline([(\"glove vectorizer\", MeanEmbeddingVectorizer(glove_big)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200, verbose=1, random_state=42, n_jobs=-1))])\n",
    "tev_glove_big = Pipeline([(\"glove vectorizer\", TfidfEmbeddingVectorizer(glove_big)), \n",
    "                        (\"extra trees\", ExtraTreesClassifier(n_estimators=200, verbose=1, random_state=42, n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now obtain the 3-fold cross-validation accuracy scores for each pipeline based on the labels 'y' (i.e. variation classes), and sort them in descending order of cross-validation accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_models = [\n",
    "    (\"mev_w2v\", mev_w2v),\n",
    "    (\"tev_w2v\", tev_w2v),\n",
    "    (\"mev_glove_small\", mev_glove_small),\n",
    "    (\"tev_glove_small\", tev_glove_small),\n",
    "    (\"mev_glove_big\", mev_glove_big),\n",
    "    (\"tev_glove_big\", tev_glove_big)\n",
    "]\n",
    "\n",
    "unsorted_scores = [(name, cross_val_score(model, X_all_text, y, cv=3, verbose=1, n_jobs=4).mean())\n",
    "                   for name, model in all_models]\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "print (tabulate(scores, floatfmt=\".4f\", headers=(\"Model\", 'Cross-Validation Accuracy Score')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[name for name, _ in scores], y=[score for _, score in scores]);\n",
    "plt.title(\"Cross-validated Accuracy Score on Training Dataset\")\n",
    "plt.xlabel(\"Pipeline\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As shown above, the <b>Mean Embedding Vectorizer</b> (mev) performs best together with the <b>Word2Vec</b> (w2v) word embedding based on the training dataset vocabulary, as it scores the highest in terms of the cross-validated accuracy score.<br>\n",
    "\n",
    "We now evaluate two options for identifying the right classifier - a Forward Neural Network (FNN) and an Extra Trees Classifier.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of (Inner) Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our single training data set (X and y) we will create two separate datasets:\n",
    "- (Inner) Training Dataset: this will be used to train our models (this will take 75% of the original training dataset)\n",
    "- Validation Dataset: this will be used to validate our trained models (e.g. check for overfitting) (this will take 25% of our total 'posts' dataset\n",
    "\n",
    "To create our datasets, we use train_test_split with the stratify option to ensure a consistent mix of values for the target feature within the created datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the original predictor and target dataframes\n",
    "X = X_original.copy()\n",
    "y = y_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2490, 4323), (2490,), (831, 4323), (831,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the indices to prevent spurious rows from appearing later during merging\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "X_val.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Mean Embedding Vectorizer object using the Word2Vec word embeddings\n",
    "mev = MeanEmbeddingVectorizer(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_mev = mev.fit(X_train['text'], y_train)\n",
    "X_train_mev = mev.transform(X_train['text'])\n",
    "X_val_mev = mev.transform(X_val['text'])\n",
    "X_test_mev = mev.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2490, 100), (831, 100), (986, 100))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mev.shape, X_val_mev.shape, X_test_mev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mev_df = pd.DataFrame(X_train_mev)\n",
    "X_val_mev_df = pd.DataFrame(X_val_mev)\n",
    "X_test_mev_df = pd.DataFrame(X_test_mev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2490, 100), (831, 100), (986, 100))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mev_df.shape, X_val_mev_df.shape, X_test_mev_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.133119</td>\n",
       "      <td>0.873349</td>\n",
       "      <td>1.633998</td>\n",
       "      <td>-0.303117</td>\n",
       "      <td>1.132908</td>\n",
       "      <td>-0.984304</td>\n",
       "      <td>0.046798</td>\n",
       "      <td>-2.390553</td>\n",
       "      <td>1.059040</td>\n",
       "      <td>-0.538725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129367</td>\n",
       "      <td>3.227998</td>\n",
       "      <td>0.304564</td>\n",
       "      <td>-1.968229</td>\n",
       "      <td>-1.260934</td>\n",
       "      <td>0.437150</td>\n",
       "      <td>0.888566</td>\n",
       "      <td>1.086563</td>\n",
       "      <td>-0.452441</td>\n",
       "      <td>-2.183851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.127070</td>\n",
       "      <td>0.836719</td>\n",
       "      <td>1.661656</td>\n",
       "      <td>-0.392819</td>\n",
       "      <td>1.247577</td>\n",
       "      <td>-0.951350</td>\n",
       "      <td>-0.060587</td>\n",
       "      <td>-2.471828</td>\n",
       "      <td>0.933894</td>\n",
       "      <td>-0.648995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125639</td>\n",
       "      <td>3.324037</td>\n",
       "      <td>0.287547</td>\n",
       "      <td>-1.951669</td>\n",
       "      <td>-1.273972</td>\n",
       "      <td>0.390522</td>\n",
       "      <td>0.819114</td>\n",
       "      <td>1.014354</td>\n",
       "      <td>-0.443538</td>\n",
       "      <td>-2.140115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.130487</td>\n",
       "      <td>0.863515</td>\n",
       "      <td>1.746012</td>\n",
       "      <td>-0.250710</td>\n",
       "      <td>1.203289</td>\n",
       "      <td>-0.995432</td>\n",
       "      <td>-0.017089</td>\n",
       "      <td>-2.371349</td>\n",
       "      <td>0.999260</td>\n",
       "      <td>-0.526616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158538</td>\n",
       "      <td>3.229475</td>\n",
       "      <td>0.334983</td>\n",
       "      <td>-1.986287</td>\n",
       "      <td>-1.264391</td>\n",
       "      <td>0.464755</td>\n",
       "      <td>0.768726</td>\n",
       "      <td>1.067215</td>\n",
       "      <td>-0.467311</td>\n",
       "      <td>-2.169511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.051054</td>\n",
       "      <td>0.796273</td>\n",
       "      <td>1.661102</td>\n",
       "      <td>-0.363374</td>\n",
       "      <td>1.174007</td>\n",
       "      <td>-0.982623</td>\n",
       "      <td>-0.036425</td>\n",
       "      <td>-2.393277</td>\n",
       "      <td>1.018895</td>\n",
       "      <td>-0.603391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071444</td>\n",
       "      <td>3.245490</td>\n",
       "      <td>0.344187</td>\n",
       "      <td>-1.931105</td>\n",
       "      <td>-1.253413</td>\n",
       "      <td>0.442468</td>\n",
       "      <td>0.807631</td>\n",
       "      <td>1.041900</td>\n",
       "      <td>-0.460288</td>\n",
       "      <td>-2.158275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.085506</td>\n",
       "      <td>0.856924</td>\n",
       "      <td>1.696304</td>\n",
       "      <td>-0.362090</td>\n",
       "      <td>1.199572</td>\n",
       "      <td>-0.998284</td>\n",
       "      <td>0.007417</td>\n",
       "      <td>-2.397368</td>\n",
       "      <td>1.025221</td>\n",
       "      <td>-0.572620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086284</td>\n",
       "      <td>3.238689</td>\n",
       "      <td>0.304584</td>\n",
       "      <td>-1.945759</td>\n",
       "      <td>-1.314536</td>\n",
       "      <td>0.428665</td>\n",
       "      <td>0.840541</td>\n",
       "      <td>1.091285</td>\n",
       "      <td>-0.466029</td>\n",
       "      <td>-2.183206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  1.133119  0.873349  1.633998 -0.303117  1.132908 -0.984304  0.046798   \n",
       "1  1.127070  0.836719  1.661656 -0.392819  1.247577 -0.951350 -0.060587   \n",
       "2  1.130487  0.863515  1.746012 -0.250710  1.203289 -0.995432 -0.017089   \n",
       "3  1.051054  0.796273  1.661102 -0.363374  1.174007 -0.982623 -0.036425   \n",
       "4  1.085506  0.856924  1.696304 -0.362090  1.199572 -0.998284  0.007417   \n",
       "\n",
       "         7         8         9   ...        90        91        92        93  \\\n",
       "0 -2.390553  1.059040 -0.538725  ...  0.129367  3.227998  0.304564 -1.968229   \n",
       "1 -2.471828  0.933894 -0.648995  ...  0.125639  3.324037  0.287547 -1.951669   \n",
       "2 -2.371349  0.999260 -0.526616  ...  0.158538  3.229475  0.334983 -1.986287   \n",
       "3 -2.393277  1.018895 -0.603391  ...  0.071444  3.245490  0.344187 -1.931105   \n",
       "4 -2.397368  1.025221 -0.572620  ...  0.086284  3.238689  0.304584 -1.945759   \n",
       "\n",
       "         94        95        96        97        98        99  \n",
       "0 -1.260934  0.437150  0.888566  1.086563 -0.452441 -2.183851  \n",
       "1 -1.273972  0.390522  0.819114  1.014354 -0.443538 -2.140115  \n",
       "2 -1.264391  0.464755  0.768726  1.067215 -0.467311 -2.169511  \n",
       "3 -1.253413  0.442468  0.807631  1.041900 -0.460288 -2.158275  \n",
       "4 -1.314536  0.428665  0.840541  1.091285 -0.466029 -2.183206  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mev_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining word vectors with dummy columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the components parts of the dataframe\n",
    "X_train = pd.concat([X_train, X_train_mev_df], axis=1)\n",
    "X_val = pd.concat([X_val, X_val_mev_df], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_mev_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=['text'], inplace=True)\n",
    "X_val.drop(columns=['text'], inplace=True)\n",
    "X_test.drop(columns=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2490, 4422), (831, 4422), (986, 4422))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the training, validation and testing datasets are now considerably smaller (approx. 4,400) - in terms of number of features - compared to those used for training the baseline model (approx. 76,000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_ABCB11</th>\n",
       "      <th>gene_ABCC6</th>\n",
       "      <th>gene_ABL1</th>\n",
       "      <th>gene_ACVR1</th>\n",
       "      <th>gene_ADAMTS13</th>\n",
       "      <th>gene_ADGRG1</th>\n",
       "      <th>gene_AGO2</th>\n",
       "      <th>gene_AGXT</th>\n",
       "      <th>gene_AKAP9</th>\n",
       "      <th>gene_AKT1</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129367</td>\n",
       "      <td>3.227998</td>\n",
       "      <td>0.304564</td>\n",
       "      <td>-1.968229</td>\n",
       "      <td>-1.260934</td>\n",
       "      <td>0.437150</td>\n",
       "      <td>0.888566</td>\n",
       "      <td>1.086563</td>\n",
       "      <td>-0.452441</td>\n",
       "      <td>-2.183851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125639</td>\n",
       "      <td>3.324037</td>\n",
       "      <td>0.287547</td>\n",
       "      <td>-1.951669</td>\n",
       "      <td>-1.273972</td>\n",
       "      <td>0.390522</td>\n",
       "      <td>0.819114</td>\n",
       "      <td>1.014354</td>\n",
       "      <td>-0.443538</td>\n",
       "      <td>-2.140115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158538</td>\n",
       "      <td>3.229475</td>\n",
       "      <td>0.334983</td>\n",
       "      <td>-1.986287</td>\n",
       "      <td>-1.264391</td>\n",
       "      <td>0.464755</td>\n",
       "      <td>0.768726</td>\n",
       "      <td>1.067215</td>\n",
       "      <td>-0.467311</td>\n",
       "      <td>-2.169511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071444</td>\n",
       "      <td>3.245490</td>\n",
       "      <td>0.344187</td>\n",
       "      <td>-1.931105</td>\n",
       "      <td>-1.253413</td>\n",
       "      <td>0.442468</td>\n",
       "      <td>0.807631</td>\n",
       "      <td>1.041900</td>\n",
       "      <td>-0.460288</td>\n",
       "      <td>-2.158275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086284</td>\n",
       "      <td>3.238689</td>\n",
       "      <td>0.304584</td>\n",
       "      <td>-1.945759</td>\n",
       "      <td>-1.314536</td>\n",
       "      <td>0.428665</td>\n",
       "      <td>0.840541</td>\n",
       "      <td>1.091285</td>\n",
       "      <td>-0.466029</td>\n",
       "      <td>-2.183206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4422 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gene_ABCB11  gene_ABCC6  gene_ABL1  gene_ACVR1  gene_ADAMTS13  gene_ADGRG1  \\\n",
       "0            0           0          0           0              0            0   \n",
       "1            0           0          0           0              0            0   \n",
       "2            0           0          0           0              0            0   \n",
       "3            0           0          0           0              0            0   \n",
       "4            0           0          0           0              0            0   \n",
       "\n",
       "   gene_AGO2  gene_AGXT  gene_AKAP9  gene_AKT1  ...        90        91  \\\n",
       "0          0          0           0          0  ...  0.129367  3.227998   \n",
       "1          0          0           0          0  ...  0.125639  3.324037   \n",
       "2          0          0           0          0  ...  0.158538  3.229475   \n",
       "3          0          0           0          0  ...  0.071444  3.245490   \n",
       "4          0          0           0          0  ...  0.086284  3.238689   \n",
       "\n",
       "         92        93        94        95        96        97        98  \\\n",
       "0  0.304564 -1.968229 -1.260934  0.437150  0.888566  1.086563 -0.452441   \n",
       "1  0.287547 -1.951669 -1.273972  0.390522  0.819114  1.014354 -0.443538   \n",
       "2  0.334983 -1.986287 -1.264391  0.464755  0.768726  1.067215 -0.467311   \n",
       "3  0.344187 -1.931105 -1.253413  0.442468  0.807631  1.041900 -0.460288   \n",
       "4  0.304584 -1.945759 -1.314536  0.428665  0.840541  1.091285 -0.466029   \n",
       "\n",
       "         99  \n",
       "0 -2.183851  \n",
       "1 -2.140115  \n",
       "2 -2.169511  \n",
       "3 -2.158275  \n",
       "4 -2.183206  \n",
       "\n",
       "[5 rows x 4422 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling of imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    0.287149\n",
       "4    0.206426\n",
       "1    0.171084\n",
       "2    0.136145\n",
       "6    0.082731\n",
       "5    0.072691\n",
       "3    0.026908\n",
       "9    0.011245\n",
       "8    0.005622\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    715\n",
       "4    514\n",
       "1    426\n",
       "2    339\n",
       "6    206\n",
       "5    181\n",
       "3     67\n",
       "9     28\n",
       "8     14\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the same oversampling strategy as per the baseline model -- i.e. to use SMOTE to oversample the 3 most infrequent classes to be 100 samples each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a SMOTE object to oversample minority classes\n",
    "sm = SMOTE(random_state=42, sampling_strategy={3:100, 9:100, 8:100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2681, 4422), (2681,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    715\n",
       "4    514\n",
       "1    426\n",
       "2    339\n",
       "6    206\n",
       "5    181\n",
       "9    100\n",
       "3    100\n",
       "8    100\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that we now have 181 samples for classes '5', '3' and '8'.\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.fit(pd.concat([X_train, X_val])) # we fit the StandardScaler based on all our training and validation data\n",
    "X_train = ms.transform(X_train)\n",
    "X_val = ms.transform(X_val)\n",
    "X_test = ms.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomised search for optimal classifier parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manage the total time and resources used to tune the classifier parameters, we use the RandomizedSearchCV to randomly select parameters from the specified ranges of parameters to give the best cross-validated accuracy score on the training dataset, with a maximum of 10 iterations. We specify the range of parameters for each classifer based on experience and past results of running the RandomizedSearchCV.\n",
    "\n",
    "We select the best classifier as the one with the highest accuracy score on the **validation dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_func(layer_one_neurons=300, layer_one_dropout=0.2,\n",
    "               layer_two_neurons=200, layer_two_dropout=0.2, opt_learning_rate=0.001):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(layer_one_neurons, activation='relu', input_dim=X_train.shape[1]))\n",
    "    model.add(Dropout(layer_one_dropout))\n",
    "\n",
    "    model.add(Dense(layer_two_neurons, activation='relu'))\n",
    "    model.add(Dropout(layer_two_dropout))\n",
    "\n",
    "    model.add(Dense(9,activation='softmax'))\n",
    "\n",
    "    Ad = optimizers.Adam(learning_rate=opt_learning_rate,\n",
    "                         beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Ad, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have selected the models below for modelling purposes.\n",
    "estimators = {\n",
    "    'nn': KerasClassifier(build_fn=model_func, epochs=10, verbose=1),\n",
    "    'svm': SVC(random_state=42),\n",
    "    'lr': LogisticRegression(random_state=42),\n",
    "    'etree': ExtraTreesClassifier(random_state=42),\n",
    "    'ada': AdaBoostClassifier(random_state=42),\n",
    "    'knn': KNeighborsClassifier(),\n",
    "    'rf': RandomForestClassifier(random_state=42),\n",
    "    'dtree': DecisionTreeClassifier(random_state=42),\n",
    "    'mnb': MultinomialNB()\n",
    "}.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'nn': {\n",
    "        'nn__layer_one_neurons': [200, 300, 400],\n",
    "        'nn__layer_one_dropout': [0.1, 0.2, 0.3],\n",
    "        'nn__layer_two_neurons' : [100, 200, 300],\n",
    "        'nn__layer_two_dropout': [0.1, 0.2, 0.3],\n",
    "        'nn__opt_learning_rate' : [0.001, 0.0001]\n",
    "    },\n",
    "    'svm': {\n",
    "        'svm__C': [0.1, 1.0, 10],\n",
    "        'svm__kernel': ['linear','poly', 'rbf', 'sigmoid'],\n",
    "        'svm__class_weight': ['balanced'] # 'balanced' will help to deal with our imbalanced classes\n",
    "    },\n",
    "    'lr': {\n",
    "        # 'liblinear' solver has been excluded as a potential solver as it cannot learn a true multinomial\n",
    "        # (multiclass) model; instead, the optimization problem is decomposed in a “one-vs-rest”\n",
    "        # fashion so separate binary classifiers are trained for all classes.\n",
    "        # 'lbfgs' solver has also been excluded as it fails to converge through past attempts\n",
    "        'lr__solver': ['sag','saga'], \n",
    "        'lr__penalty': ['l1', 'l2'],\n",
    "        'lr__C': [0.1, 1.0, 10],\n",
    "        'lr__max_iter': [50], # limit the max. no. of iterations to 50 to speed up processing\n",
    "        'lr__multi_class': ['multinomial'],\n",
    "        'lr__class_weight': ['balanced'] # 'balanced' will help to deal with our imbalanced classes\n",
    "    },\n",
    "    'etree': {\n",
    "        'etree__max_features': ['auto', 'sqrt', 'log2', None],\n",
    "        'etree__min_samples_split': [4, 6, 8],\n",
    "        'etree__min_samples_leaf': [2, 3, 4],\n",
    "        'etree__class_weight': ['balanced'] # 'balanced' will help to deal with our imbalanced classes\n",
    "    },\n",
    "    'ada': {\n",
    "        'ada__n_estimators': [50, 100, 150],\n",
    "        'ada__learning_rate': [1, 1.5, 2]\n",
    "    },\n",
    "    'knn': {\n",
    "        'knn__n_neighbors': [3, 5, 7]\n",
    "    },\n",
    "    'rf': {\n",
    "        'rf__n_estimators': [100, 200, 300],\n",
    "        'rf__class_weight': ['balanced'], # 'balanced' will help to deal with our imbalanced classes\n",
    "        'rf__min_samples_split': [5, 10, 15],\n",
    "        'rf__min_samples_leaf': [2, 3, 4]\n",
    "    },\n",
    "    'dtree': {\n",
    "        'dtree__max_features': ['auto', 'sqrt', 'log2', None],\n",
    "        'dtree__min_samples_split': [4, 6, 8],\n",
    "        'dtree__min_samples_leaf': [2, 3, 4],\n",
    "        'dtree__class_weight': ['balanced'] # 'balanced' will help to deal with our imbalanced classes\n",
    "    },\n",
    "    'mnb': {\n",
    "        'mnb__alpha': [0, 0.5, 1.0],\n",
    "        'mnb__fit_prior': [True, False]  \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use RandomizedSearchCV to select the optimal parameters for each classifier that produces the best 3-fold cross-validated mean accuracy score based on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  nn\n",
      "Started fitting at: 2020-04-07 18:24:29\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yuchy\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Epoch 1/10\n",
      "1787/1787 [==============================] - 4s 2ms/step - loss: 2.0407 - accuracy: 0.2451\n",
      "Epoch 2/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 1.9488 - accuracy: 0.3195\n",
      "Epoch 3/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 1.7158 - accuracy: 0.4203\n",
      "Epoch 4/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 1.4622 - accuracy: 0.4952\n",
      "Epoch 5/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 1.2644 - accuracy: 0.5585\n",
      "Epoch 6/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 1.0540 - accuracy: 0.6391\n",
      "Epoch 7/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.9147 - accuracy: 0.7073\n",
      "Epoch 8/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.7530 - accuracy: 0.7655\n",
      "Epoch 9/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.6027 - accuracy: 0.8142\n",
      "Epoch 10/10\n",
      "1787/1787 [==============================] - 3s 1ms/step - loss: 0.4717 - accuracy: 0.8562\n",
      "894/894 [==============================] - 0s 356us/step\n",
      "1787/1787 [==============================] - 1s 280us/step\n",
      "Epoch 1/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 2.0568 - accuracy: 0.2289\n",
      "Epoch 2/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 1.9670 - accuracy: 0.2832\n",
      "Epoch 3/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 1.7858 - accuracy: 0.4096\n",
      "Epoch 4/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 1.5036 - accuracy: 0.4924\n",
      "Epoch 5/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 1.2760 - accuracy: 0.5574\n",
      "Epoch 6/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 1.1176 - accuracy: 0.6284\n",
      "Epoch 7/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.9206 - accuracy: 0.6905\n",
      "Epoch 8/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.7579 - accuracy: 0.7655\n",
      "Epoch 9/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.6207 - accuracy: 0.8075\n",
      "Epoch 10/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.5094 - accuracy: 0.8523\n",
      "894/894 [==============================] - 0s 452us/step\n",
      "1787/1787 [==============================] - 0s 265us/step\n",
      "Epoch 1/10\n",
      "1788/1788 [==============================] - 3s 2ms/step - loss: 1.8962 - accuracy: 0.2673\n",
      "Epoch 2/10\n",
      "1788/1788 [==============================] - 3s 2ms/step - loss: 1.7589 - accuracy: 0.3479\n",
      "Epoch 3/10\n",
      "1788/1788 [==============================] - 3s 2ms/step - loss: 1.5357 - accuracy: 0.4771\n",
      "Epoch 4/10\n",
      "1788/1788 [==============================] - 3s 2ms/step - loss: 1.2568 - accuracy: 0.5671\n",
      "Epoch 5/10\n",
      "1788/1788 [==============================] - 3s 2ms/step - loss: 1.0742 - accuracy: 0.6348\n",
      "Epoch 6/10\n",
      "1788/1788 [==============================] - 3s 2ms/step - loss: 0.9123 - accuracy: 0.6868: 0s - loss: 0.912\n",
      "Epoch 7/10\n",
      "1788/1788 [==============================] - 3s 2ms/step - loss: 0.7897 - accuracy: 0.7321\n",
      "Epoch 8/10\n",
      "1788/1788 [==============================] - 3s 2ms/step - loss: 0.6041 - accuracy: 0.8104\n",
      "Epoch 9/10\n",
      "1788/1788 [==============================] - 3s 2ms/step - loss: 0.4804 - accuracy: 0.8557\n",
      "Epoch 10/10\n",
      "1788/1788 [==============================] - 3s 2ms/step - loss: 0.3589 - accuracy: 0.9111\n",
      "893/893 [==============================] - 1s 612us/step\n",
      "1788/1788 [==============================] - 1s 302us/step\n",
      "Epoch 1/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 2.1097 - accuracy: 0.2238\n",
      "Epoch 2/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 2.0196 - accuracy: 0.2658\n",
      "Epoch 3/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 1.9991 - accuracy: 0.2720\n",
      "Epoch 4/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 1.9771 - accuracy: 0.2708\n",
      "Epoch 5/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 1.9764 - accuracy: 0.2910\n",
      "Epoch 6/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 1.9554 - accuracy: 0.2960\n",
      "Epoch 7/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 1.9429 - accuracy: 0.3011\n",
      "Epoch 8/10\n",
      "1787/1787 [==============================] - 2s 974us/step - loss: 1.9124 - accuracy: 0.3184\n",
      "Epoch 9/10\n",
      "1787/1787 [==============================] - 1s 691us/step - loss: 1.8985 - accuracy: 0.3514\n",
      "Epoch 10/10\n",
      "1787/1787 [==============================] - 1s 683us/step - loss: 1.8651 - accuracy: 0.36540s - loss: 1.8667 - accuracy: 0.36\n",
      "894/894 [==============================] - 0s 340us/step\n",
      "1787/1787 [==============================] - 0s 145us/step\n",
      "Epoch 1/10\n",
      "1787/1787 [==============================] - 1s 514us/step - loss: 2.0966 - accuracy: 0.2406\n",
      "Epoch 2/10\n",
      "1787/1787 [==============================] - 1s 535us/step - loss: 2.0277 - accuracy: 0.2490\n",
      "Epoch 3/10\n",
      "1787/1787 [==============================] - 1s 584us/step - loss: 2.0069 - accuracy: 0.2636\n",
      "Epoch 4/10\n",
      "1787/1787 [==============================] - 1s 552us/step - loss: 1.9961 - accuracy: 0.2781\n",
      "Epoch 5/10\n",
      "1787/1787 [==============================] - 1s 520us/step - loss: 1.9822 - accuracy: 0.2748\n",
      "Epoch 6/10\n",
      "1787/1787 [==============================] - 1s 575us/step - loss: 1.9757 - accuracy: 0.2927\n",
      "Epoch 7/10\n",
      "1787/1787 [==============================] - 1s 646us/step - loss: 1.9499 - accuracy: 0.3022\n",
      "Epoch 8/10\n",
      "1787/1787 [==============================] - 1s 578us/step - loss: 1.9267 - accuracy: 0.33520s - loss: 1.926\n",
      "Epoch 9/10\n",
      "1787/1787 [==============================] - 1s 604us/step - loss: 1.9048 - accuracy: 0.3430\n",
      "Epoch 10/10\n",
      "1787/1787 [==============================] - 1s 683us/step - loss: 1.8672 - accuracy: 0.3755\n",
      "894/894 [==============================] - 0s 403us/step\n",
      "1787/1787 [==============================] - 0s 112us/step\n",
      "Epoch 1/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 2.0328 - accuracy: 0.2232\n",
      "Epoch 2/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.8864 - accuracy: 0.2813\n",
      "Epoch 3/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.8626 - accuracy: 0.2763\n",
      "Epoch 4/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.8429 - accuracy: 0.2780: 0s - loss: 1.8342 - ac\n",
      "Epoch 5/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.8213 - accuracy: 0.2847\n",
      "Epoch 6/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.8200 - accuracy: 0.3059\n",
      "Epoch 7/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.7923 - accuracy: 0.3289\n",
      "Epoch 8/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.7610 - accuracy: 0.3607\n",
      "Epoch 9/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.7450 - accuracy: 0.3658\n",
      "Epoch 10/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.7050 - accuracy: 0.4032\n",
      "893/893 [==============================] - 1s 898us/step\n",
      "1788/1788 [==============================] - 0s 233us/step\n",
      "Epoch 1/10\n",
      "1787/1787 [==============================] - 3s 1ms/step - loss: 2.0885 - accuracy: 0.2250: 0s - loss: 2.0887 - accuracy: 0.22\n",
      "Epoch 2/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 2.0009 - accuracy: 0.2669\n",
      "Epoch 3/10\n",
      "1787/1787 [==============================] - 3s 1ms/step - loss: 1.9869 - accuracy: 0.2680\n",
      "Epoch 4/10\n",
      "1787/1787 [==============================] - 3s 1ms/step - loss: 1.9699 - accuracy: 0.2725\n",
      "Epoch 5/10\n",
      "1787/1787 [==============================] - 3s 1ms/step - loss: 1.9427 - accuracy: 0.3089\n",
      "Epoch 6/10\n",
      "1787/1787 [==============================] - 3s 1ms/step - loss: 1.9126 - accuracy: 0.3218\n",
      "Epoch 7/10\n",
      "1787/1787 [==============================] - 3s 1ms/step - loss: 1.8589 - accuracy: 0.3671\n",
      "Epoch 8/10\n",
      "1787/1787 [==============================] - 3s 1ms/step - loss: 1.8076 - accuracy: 0.4113\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1787/1787 [==============================] - 2s 1ms/step - loss: 1.7531 - accuracy: 0.4466\n",
      "Epoch 10/10\n",
      "1787/1787 [==============================] - 3s 1ms/step - loss: 1.6800 - accuracy: 0.4673\n",
      "894/894 [==============================] - 1s 967us/step\n",
      "1787/1787 [==============================] - 1s 320us/step\n",
      "Epoch 1/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 2.0944 - accuracy: 0.2210\n",
      "Epoch 2/10\n",
      "1787/1787 [==============================] - 3s 1ms/step - loss: 2.0146 - accuracy: 0.2552\n",
      "Epoch 3/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 1.9931 - accuracy: 0.2597\n",
      "Epoch 4/10\n",
      "1787/1787 [==============================] - 3s 1ms/step - loss: 1.9735 - accuracy: 0.2921\n",
      "Epoch 5/10\n",
      "1787/1787 [==============================] - 3s 1ms/step - loss: 1.9503 - accuracy: 0.2977\n",
      "Epoch 6/10\n",
      "1787/1787 [==============================] - 3s 1ms/step - loss: 1.9093 - accuracy: 0.3442\n",
      "Epoch 7/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 1.8687 - accuracy: 0.3677\n",
      "Epoch 8/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 1.8061 - accuracy: 0.4012\n",
      "Epoch 9/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 1.7485 - accuracy: 0.4348\n",
      "Epoch 10/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 1.6677 - accuracy: 0.4656\n",
      "894/894 [==============================] - 1s 894us/step\n",
      "1787/1787 [==============================] - 1s 286us/step\n",
      "Epoch 1/10\n",
      "1788/1788 [==============================] - 3s 2ms/step - loss: 2.0053 - accuracy: 0.2578\n",
      "Epoch 2/10\n",
      "1788/1788 [==============================] - ETA: 0s - loss: 1.8648 - accuracy: 0.27 - 3s 1ms/step - loss: 1.8599 - accuracy: 0.2813\n",
      "Epoch 3/10\n",
      "1788/1788 [==============================] - 3s 2ms/step - loss: 1.8258 - accuracy: 0.2880\n",
      "Epoch 4/10\n",
      "1788/1788 [==============================] - 3s 1ms/step - loss: 1.8086 - accuracy: 0.3070\n",
      "Epoch 5/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.7806 - accuracy: 0.3395\n",
      "Epoch 6/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.7466 - accuracy: 0.3518\n",
      "Epoch 7/10\n",
      "1788/1788 [==============================] - 3s 1ms/step - loss: 1.7024 - accuracy: 0.4122\n",
      "Epoch 8/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.6465 - accuracy: 0.4609\n",
      "Epoch 9/10\n",
      "1788/1788 [==============================] - 3s 1ms/step - loss: 1.5857 - accuracy: 0.4994\n",
      "Epoch 10/10\n",
      "1788/1788 [==============================] - 3s 1ms/step - loss: 1.5181 - accuracy: 0.5218\n",
      "893/893 [==============================] - 1s 1ms/step\n",
      "1788/1788 [==============================] - 1s 302us/step\n",
      "Epoch 1/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 2.1062 - accuracy: 0.2227\n",
      "Epoch 2/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 2.0127 - accuracy: 0.2658: 0s - loss: 2.0036 - accuracy\n",
      "Epoch 3/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 2.0048 - accuracy: 0.2664\n",
      "Epoch 4/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 1.9838 - accuracy: 0.2720\n",
      "Epoch 5/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 1.9676 - accuracy: 0.2703\n",
      "Epoch 6/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 1.9487 - accuracy: 0.2792\n",
      "Epoch 7/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 1.9233 - accuracy: 0.2988\n",
      "Epoch 8/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 1.8892 - accuracy: 0.3184\n",
      "Epoch 9/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 1.8708 - accuracy: 0.3794\n",
      "Epoch 10/10\n",
      "1787/1787 [==============================] - 2s 1ms/step - loss: 1.8223 - accuracy: 0.3816: 1s\n",
      "894/894 [==============================] - 1s 1ms/step\n",
      "1787/1787 [==============================] - 1s 286us/step\n",
      "Epoch 1/10\n",
      "1787/1787 [==============================] - 1s 763us/step - loss: 2.0809 - accuracy: 0.2451\n",
      "Epoch 2/10\n",
      "1787/1787 [==============================] - 1s 692us/step - loss: 2.0152 - accuracy: 0.2669\n",
      "Epoch 3/10\n",
      "1787/1787 [==============================] - 1s 705us/step - loss: 1.9991 - accuracy: 0.2675\n",
      "Epoch 4/10\n",
      "1787/1787 [==============================] - 1s 525us/step - loss: 1.9687 - accuracy: 0.2882\n",
      "Epoch 5/10\n",
      "1787/1787 [==============================] - 1s 515us/step - loss: 1.9519 - accuracy: 0.2983\n",
      "Epoch 6/10\n",
      "1787/1787 [==============================] - 1s 510us/step - loss: 1.9177 - accuracy: 0.3486\n",
      "Epoch 7/10\n",
      "1787/1787 [==============================] - 1s 518us/step - loss: 1.8836 - accuracy: 0.3649\n",
      "Epoch 8/10\n",
      "1787/1787 [==============================] - 1s 511us/step - loss: 1.8469 - accuracy: 0.40680s - loss: 1.8545 - accuracy: \n",
      "Epoch 9/10\n",
      "1787/1787 [==============================] - 1s 599us/step - loss: 1.8004 - accuracy: 0.4231\n",
      "Epoch 10/10\n",
      "1787/1787 [==============================] - 1s 555us/step - loss: 1.7480 - accuracy: 0.4348\n",
      "894/894 [==============================] - 0s 490us/step\n",
      "1787/1787 [==============================] - 0s 114us/step\n",
      "Epoch 1/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 2.0282 - accuracy: 0.2438\n",
      "Epoch 2/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.8869 - accuracy: 0.2836\n",
      "Epoch 3/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.8415 - accuracy: 0.2864\n",
      "Epoch 4/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.8273 - accuracy: 0.3003\n",
      "Epoch 5/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.8096 - accuracy: 0.3205\n",
      "Epoch 6/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.7842 - accuracy: 0.3484\n",
      "Epoch 7/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.7608 - accuracy: 0.3596\n",
      "Epoch 8/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.7375 - accuracy: 0.3988\n",
      "Epoch 9/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.6840 - accuracy: 0.4044\n",
      "Epoch 10/10\n",
      "1788/1788 [==============================] - 2s 1ms/step - loss: 1.6507 - accuracy: 0.4463\n",
      "893/893 [==============================] - 1s 1ms/step\n",
      "1788/1788 [==============================] - 1s 298us/step\n",
      "Epoch 1/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 2.0173 - accuracy: 0.2608\n",
      "Epoch 2/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 1.8594 - accuracy: 0.3632\n",
      "Epoch 3/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 1.5177 - accuracy: 0.4807\n",
      "Epoch 4/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 1.2478 - accuracy: 0.5736\n",
      "Epoch 5/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.9824 - accuracy: 0.6637\n",
      "Epoch 6/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.7712 - accuracy: 0.7644\n",
      "Epoch 7/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.5782 - accuracy: 0.8265\n",
      "Epoch 8/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.4395 - accuracy: 0.8719\n",
      "Epoch 9/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.3116 - accuracy: 0.9172\n",
      "Epoch 10/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.2149 - accuracy: 0.9480\n",
      "894/894 [==============================] - 1s 2ms/step\n",
      "1787/1787 [==============================] - 1s 379us/step\n",
      "Epoch 1/10\n",
      "1787/1787 [==============================] - 4s 2ms/step - loss: 2.0093 - accuracy: 0.2703: 1s\n",
      "Epoch 2/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 1.7917 - accuracy: 0.3968\n",
      "Epoch 3/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 1.4377 - accuracy: 0.5132\n",
      "Epoch 4/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 1.1317 - accuracy: 0.6072\n",
      "Epoch 5/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.8940 - accuracy: 0.7079\n",
      "Epoch 6/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.6669 - accuracy: 0.7946\n",
      "Epoch 7/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.4925 - accuracy: 0.8674\n",
      "Epoch 8/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.4107 - accuracy: 0.8702\n",
      "Epoch 9/10\n",
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.2653 - accuracy: 0.9301\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1787/1787 [==============================] - 3s 2ms/step - loss: 0.1903 - accuracy: 0.9457\n",
      "894/894 [==============================] - 1s 2ms/step\n",
      "1787/1787 [==============================] - 1s 396us/step\n",
      "Epoch 1/10\n",
      "1788/1788 [==============================] - 4s 2ms/step - loss: 1.8718 - accuracy: 0.2645\n",
      "Epoch 2/10\n",
      "1788/1788 [==============================] - 3s 2ms/step - loss: 1.6942 - accuracy: 0.3865\n",
      "Epoch 3/10\n",
      "1788/1788 [==============================] - 3s 2ms/step - loss: 1.3312 - accuracy: 0.5324: 0s - loss: 1.3645 - accu\n",
      "Epoch 4/10\n",
      "1788/1788 [==============================] - 3s 2ms/step - loss: 1.0792 - accuracy: 0.6370\n",
      "Epoch 5/10\n",
      "1216/1788 [===================>..........] - ETA: 1s - loss: 0.8677 - accuracy: 0.7064"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initialise empty lists to store information later\n",
    "models = []\n",
    "parameters = []\n",
    "best_score = []\n",
    "train_bal_f1_score = []\n",
    "val_bal_f1_score = []\n",
    "train_bal_accuracy = []\n",
    "val_bal_accuracy = []\n",
    "\n",
    "for k,v in estimators:\n",
    "    start = time.time()\n",
    "    pipe = Pipeline([(k,v)])\n",
    "    param = params[k]\n",
    "    randomsearch = RandomizedSearchCV(\n",
    "        n_iter = 10, # we set a max. of 10 iterations\n",
    "        estimator = pipe,\n",
    "        random_state = 42,\n",
    "        param_distributions = param,\n",
    "        verbose = 1,\n",
    "        cv = 3, # 3-fold cross-validation\n",
    "        n_jobs=None,\n",
    "        return_train_score= True,\n",
    "        scoring = 'accuracy' # use best cross-validation accuracy score\n",
    "    )\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    print (\"Model: \", k)\n",
    "    print (\"Started fitting at: {}\".format(now.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "    \n",
    "    randomsearch.fit(X_train, y_train)\n",
    "    \n",
    "    model = randomsearch.best_estimator_\n",
    "    cv_score = randomsearch.cv_results_\n",
    "    best_params = randomsearch.best_params_\n",
    "\n",
    "    # predict y\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # print results\n",
    "    print (\"Fitting duration (h:m:s): {}\".format(str(datetime.timedelta(seconds=time.time()-start))))\n",
    "    print (\"Best parameters:\", best_params)\n",
    "    print (\"Best cross-validation accuracy score:\", randomsearch.best_score_)\n",
    "    print (\"Training set balanced F1 score:\", f1_score(y_train, y_train_pred, average='weighted'))\n",
    "    print (\"Validation set balanced F1 score:\", f1_score(y_val, y_val_pred, average='weighted'))\n",
    "    print (\"Training set balanced accuracy:\", balanced_accuracy_score(y_train,y_train_pred))\n",
    "    print (\"Validation set balanced accuracy:\", balanced_accuracy_score(y_val,y_val_pred))\n",
    "    print (\"\")\n",
    "    \n",
    "    # append info to list\n",
    "    models.append(k)\n",
    "    best_score.append(randomsearch.best_score_)\n",
    "    parameters.append(best_params)\n",
    "    train_bal_f1_score.append(f1_score(y_train,y_train_pred, average='weighted'))\n",
    "    val_bal_f1_score.append(f1_score(y_val,y_val_pred, average='weighted'))\n",
    "    train_bal_accuracy.append(balanced_accuracy_score(y_train,y_train_pred))\n",
    "    val_bal_accuracy.append(balanced_accuracy_score(y_val,y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirmation of Alternative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>parameters</th>\n",
       "      <th>Best cross-validation accuracy score</th>\n",
       "      <th>Training set balanced F1 score</th>\n",
       "      <th>Validation set balanced F1 score</th>\n",
       "      <th>Training set balanced accuracy</th>\n",
       "      <th>Validation set balanced accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>etree</td>\n",
       "      <td>{'etree__min_samples_split': 4, 'etree__min_samples_leaf': 3, 'etree__max_features': None, 'etree__class_weight': 'balanced'}</td>\n",
       "      <td>0.652742</td>\n",
       "      <td>0.900900</td>\n",
       "      <td>0.629522</td>\n",
       "      <td>0.923030</td>\n",
       "      <td>0.544572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf</td>\n",
       "      <td>{'rf__n_estimators': 100, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__class_weight': 'balanced'}</td>\n",
       "      <td>0.615817</td>\n",
       "      <td>0.875395</td>\n",
       "      <td>0.594935</td>\n",
       "      <td>0.900788</td>\n",
       "      <td>0.517614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svm</td>\n",
       "      <td>{'svm__kernel': 'linear', 'svm__class_weight': 'balanced', 'svm__C': 10}</td>\n",
       "      <td>0.578148</td>\n",
       "      <td>0.995525</td>\n",
       "      <td>0.538901</td>\n",
       "      <td>0.994418</td>\n",
       "      <td>0.474786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nn</td>\n",
       "      <td>{'nn__opt_learning_rate': 0.0001, 'nn__layer_two_neurons': 3000, 'nn__layer_two_dropout': 0.2, 'nn__layer_one_neurons': 4000, 'nn__layer_one_dropout': 0.2}</td>\n",
       "      <td>0.545310</td>\n",
       "      <td>0.986804</td>\n",
       "      <td>0.489068</td>\n",
       "      <td>0.963534</td>\n",
       "      <td>0.456743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lr</td>\n",
       "      <td>{'lr__solver': 'sag', 'lr__penalty': 'l2', 'lr__multi_class': 'multinomial', 'lr__max_iter': 50, 'lr__class_weight': 'balanced', 'lr__C': 10}</td>\n",
       "      <td>0.573663</td>\n",
       "      <td>0.898156</td>\n",
       "      <td>0.514490</td>\n",
       "      <td>0.917066</td>\n",
       "      <td>0.448596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>knn</td>\n",
       "      <td>{'knn__n_neighbors': 3}</td>\n",
       "      <td>0.486782</td>\n",
       "      <td>0.671897</td>\n",
       "      <td>0.502186</td>\n",
       "      <td>0.694450</td>\n",
       "      <td>0.425505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dtree</td>\n",
       "      <td>{'dtree__min_samples_split': 4, 'dtree__min_samples_leaf': 3, 'dtree__max_features': None, 'dtree__class_weight': 'balanced'}</td>\n",
       "      <td>0.493475</td>\n",
       "      <td>0.803187</td>\n",
       "      <td>0.484929</td>\n",
       "      <td>0.864567</td>\n",
       "      <td>0.418237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mnb</td>\n",
       "      <td>{'mnb__fit_prior': False, 'mnb__alpha': 0.5}</td>\n",
       "      <td>0.493844</td>\n",
       "      <td>0.522038</td>\n",
       "      <td>0.451758</td>\n",
       "      <td>0.349032</td>\n",
       "      <td>0.266541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ada</td>\n",
       "      <td>{'ada__n_estimators': 50, 'ada__learning_rate': 1}</td>\n",
       "      <td>0.336808</td>\n",
       "      <td>0.243761</td>\n",
       "      <td>0.228112</td>\n",
       "      <td>0.219612</td>\n",
       "      <td>0.205564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model  \\\n",
       "0  etree   \n",
       "1     rf   \n",
       "2    svm   \n",
       "3     nn   \n",
       "4     lr   \n",
       "5    knn   \n",
       "6  dtree   \n",
       "7    mnb   \n",
       "8    ada   \n",
       "\n",
       "                                                                                                                                                    parameters  \\\n",
       "0                                {'etree__min_samples_split': 4, 'etree__min_samples_leaf': 3, 'etree__max_features': None, 'etree__class_weight': 'balanced'}   \n",
       "1                                             {'rf__n_estimators': 100, 'rf__min_samples_split': 5, 'rf__min_samples_leaf': 2, 'rf__class_weight': 'balanced'}   \n",
       "2                                                                                     {'svm__kernel': 'linear', 'svm__class_weight': 'balanced', 'svm__C': 10}   \n",
       "3  {'nn__opt_learning_rate': 0.0001, 'nn__layer_two_neurons': 3000, 'nn__layer_two_dropout': 0.2, 'nn__layer_one_neurons': 4000, 'nn__layer_one_dropout': 0.2}   \n",
       "4                {'lr__solver': 'sag', 'lr__penalty': 'l2', 'lr__multi_class': 'multinomial', 'lr__max_iter': 50, 'lr__class_weight': 'balanced', 'lr__C': 10}   \n",
       "5                                                                                                                                      {'knn__n_neighbors': 3}   \n",
       "6                                {'dtree__min_samples_split': 4, 'dtree__min_samples_leaf': 3, 'dtree__max_features': None, 'dtree__class_weight': 'balanced'}   \n",
       "7                                                                                                                 {'mnb__fit_prior': False, 'mnb__alpha': 0.5}   \n",
       "8                                                                                                           {'ada__n_estimators': 50, 'ada__learning_rate': 1}   \n",
       "\n",
       "   Best cross-validation accuracy score  Training set balanced F1 score  \\\n",
       "0                              0.652742                        0.900900   \n",
       "1                              0.615817                        0.875395   \n",
       "2                              0.578148                        0.995525   \n",
       "3                              0.545310                        0.986804   \n",
       "4                              0.573663                        0.898156   \n",
       "5                              0.486782                        0.671897   \n",
       "6                              0.493475                        0.803187   \n",
       "7                              0.493844                        0.522038   \n",
       "8                              0.336808                        0.243761   \n",
       "\n",
       "   Validation set balanced F1 score  Training set balanced accuracy  \\\n",
       "0                          0.629522                        0.923030   \n",
       "1                          0.594935                        0.900788   \n",
       "2                          0.538901                        0.994418   \n",
       "3                          0.489068                        0.963534   \n",
       "4                          0.514490                        0.917066   \n",
       "5                          0.502186                        0.694450   \n",
       "6                          0.484929                        0.864567   \n",
       "7                          0.451758                        0.349032   \n",
       "8                          0.228112                        0.219612   \n",
       "\n",
       "   Validation set balanced accuracy  \n",
       "0                          0.544572  \n",
       "1                          0.517614  \n",
       "2                          0.474786  \n",
       "3                          0.456743  \n",
       "4                          0.448596  \n",
       "5                          0.425505  \n",
       "6                          0.418237  \n",
       "7                          0.266541  \n",
       "8                          0.205564  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Produce a summary table of the tuned classifiers\n",
    "summary = pd.DataFrame({\n",
    "    'model': models,\n",
    "    'parameters': parameters,\n",
    "    'Best cross-validation accuracy score': best_score,\n",
    "    'Training set balanced F1 score': train_bal_f1_score,\n",
    "    'Validation set balanced F1 score': val_bal_f1_score,\n",
    "    'Training set balanced accuracy': train_bal_accuracy,\n",
    "    'Validation set balanced accuracy': val_bal_accuracy\n",
    "    })\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "summary.sort_values('Validation set balanced accuracy', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the table above, we observe the following:\n",
    "- The best performing model has a validation dataset balanced accuracy score of `0.544572` which is better than our baseline accuracy score of `0.286962`.\n",
    "- For all the models, the balanced F1 and accuracy scores for the validation dataset are lower that the respective scores on the training dataset -- this indicates that all our models tend to be overfitted.\n",
    "- Using the validation dataset balanced accuracy score as our selection metric, we see that the Extra Trees Classifier is the best performing classifier (`0.544572`). This score is <b>lower</b> than the baseline model (`xxx`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "In conclusion, our best alternative model is therefore the <b>Extra Trees classifier</b> trained on <b>Word2Vec word embeddings</b> using the <b>Mean Embedding Vectorizer</b>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Exploration of Alternative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We instantiate the best classifier based on the best parameters found above\n",
    "alternative_clf = ExtraTreesClassifier(verbose=1, n_jobs=4, random_state=42, n_estimators=250,\n",
    "                                       min_samples_split=4, min_samples_leaf=2, max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the best classifier on the training dataset\n",
    "alternative_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions for the validation data based on our baseline model\n",
    "y_val_pred = alternative_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the output\n",
    "y_train_binarized = label_binarize(y_train, classes=list(np.unique(y)))\n",
    "y_val_pred_binarized = label_binarize(y_val_pred, classes=list(np.unique(y)))\n",
    "n_classes = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "now = datetime.datetime.now()\n",
    "print (\"Started fitting at: {}\".format(now.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "# We obtain the distances of each sample from the decision boundary for each class\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n",
    "                                 random_state=42, verbose=1), n_jobs=2)\n",
    "y_score = classifier.fit(X_train, y_train_binarized).decision_function(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    # we compare our predicted labels for the validation dataset and the actual validation dataset labels\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_val_pred_binarized[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_val_pred_binarized.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot ROC curves for all the 9 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "lw=2\n",
    "colors = cycle(['rosybrown', 'firebrick', 'sienna', 'olivedrab', 'darkgreen',\\\n",
    "                'lightseagreen', 'darkturquoise', 'b', 'darkorange'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i+1, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Multiple Classes')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe above that the AUC scores for all the 9 classes are very high, even though the classes in the original training dataset were highly imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize class distribution\n",
    "print (Counter(y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Frequency Distribution of Predicted Classes for Validation Dataset\")\n",
    "sns.countplot(y_val_pred)\n",
    "plt.xlabel(\"Predicted Class\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "target_names = ['class 1', 'class 2', 'class 3', 'class 4', 'class 5',\n",
    "                'class 6', 'class 7', 'class 8', 'class 9']\n",
    "print(classification_report(y_val, y_val_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now display the balanced accuracy based on the validation dataset, which can cater to imbalanced datasets. It is the macro-average of recall scores per class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Alternative Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the training data has been reduced significantly from over 76,000 features (created using the standard TfidfVectorizer) to around 4,400, which is approx. 20 times smaller. This has led to faster training times for the alternative model.\n",
    "\n",
    "Unfortunately, variance has increased as a result, and the alternative model is more overfitted compared to the baseline model, based on a comparison of the validation dataset balanced accuracy scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Export (for Kaggle Submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = alternative_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the 'id' column since we need this for the Kaggle submission\n",
    "test_pred = pd.concat([test['id'], pd.DataFrame(y_test_pred, columns=['class'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that we have a mix of predictions for variation classes\n",
    "test_pred['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We export the predictions\n",
    "test_pred.to_csv(\"../assets/test_pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
